{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5da988",
      "metadata": {
        "id": "fc5da988"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "187cfa54",
      "metadata": {
        "id": "187cfa54"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, scale_factor=1, dropout=0.0, fill=float('-inf')):\n",
        "        super(ScaledDotProductAttention).__init__()\n",
        "        self.scale_factor = scale_factor\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.fill = fill\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask=None):\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) / self.scale_factor\n",
        "\n",
        "        if attn_mask is not None:\n",
        "            attn = attn.masked_fill(attn_mask == 0, self.fill)\n",
        "\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.dropout(attn)\n",
        "        y = torch.matmul(attn, v)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2df98683",
      "metadata": {
        "id": "2df98683"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, block_size=1024, dropout=0.1, eps=1e-6):\n",
        "        super(MultiHeadAttention).__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.block_size = block_size\n",
        "\n",
        "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim, bias=False)\n",
        "        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "        scale_factor = (self.embed_dim // self.num_heads) ** 0.5\n",
        "        self.attn = ScaledDotProductAttention(scale_factor=scale_factor, dropout=dropout)\n",
        "\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(block_size, block_size)).view(1, 1, block_size, block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        q, k ,v  = self.qkv_proj(x).split(self.embed_dim, dim=2)\n",
        "        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "        y = self.attn(q, k, v, self.bias[:, :, :T, :T])\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.output_proj(y))\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd50ded",
      "metadata": {
        "id": "2bd50ded"
      },
      "outputs": [],
      "source": [
        "class NewGELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NewGELU).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, embed_dim, dropout=0.1):\n",
        "        super(MLP).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            NewGELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86c8416",
      "metadata": {
        "id": "f86c8416"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, block_size=1024, dropout=0.1, eps=1e-6):\n",
        "        super(Block).__init__()\n",
        "        self.sub_block_1 = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            MultiHeadAttention(embed_dim, num_heads, block_size, dropout, eps)\n",
        "        )\n",
        "\n",
        "        self.sub_block_2 = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            MLP(embed_dim, dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sub_block_1(x)\n",
        "        x = x + self.sub_block_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c55500",
      "metadata": {
        "id": "a5c55500"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}