{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Byok33X1AazJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLHhpUq7fFk0",
        "cellView": "form"
      },
      "source": [
        "#@title Dependencies\n",
        "!pip install numpy\n",
        "!pip install gym\n",
        "!pip install matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEkG6HGLfvMJ",
        "cellView": "form"
      },
      "source": [
        "#@title Setup Environments\n",
        "\n",
        "import numpy as np\n",
        "import gym\n",
        "import sys\n",
        "from six import StringIO, b\n",
        "from gym.envs.toy_text import frozen_lake, discrete\n",
        "from gym.envs.registration import register\n",
        "import time\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=np.inf)\n",
        "\n",
        "\n",
        "def categorical_sample(prob_n, np_random):\n",
        "    \"\"\"\n",
        "    Sample from categorical distribution\n",
        "    Each row specifies class probabilities\n",
        "    \"\"\"\n",
        "    prob_n = np.asarray(prob_n)\n",
        "    csprob_n = np.cumsum(prob_n)\n",
        "    return (csprob_n > np_random.rand()).argmax()\n",
        "\n",
        "\n",
        "class DiscreteEnv(gym.Env):\n",
        "\n",
        "    \"\"\"\n",
        "    Has the following members\n",
        "    - nS: number of states\n",
        "    - nA: number of actions\n",
        "    - P: transitions (*)\n",
        "    - isd: initial state distribution (**)\n",
        "\n",
        "    (*) dictionary dict of dicts of lists, where\n",
        "      P[s][a] == [(probability, nextstate, reward, done), ...]\n",
        "    (**) list or array of length nS\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, nS, nA, P, isd):\n",
        "        self.P = P\n",
        "        self.isd = isd\n",
        "        self.lastaction=None # for rendering\n",
        "        self.nS = nS\n",
        "        self.nA = nA\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.nA)\n",
        "        self.observation_space = spaces.Discrete(self.nS)\n",
        "\n",
        "        self._seed()\n",
        "        self._reset()\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def _reset(self):\n",
        "        self.s = categorical_sample(self.isd, self.np_random)\n",
        "        self.lastaction=None\n",
        "        return self.s\n",
        "\n",
        "    def _step(self, a):\n",
        "        transitions = self.P[self.s][a]\n",
        "        i = categorical_sample([t[0] for t in transitions], self.np_random)\n",
        "        p, s, r, d= transitions[i]\n",
        "        self.s = s\n",
        "        self.lastaction=a\n",
        "        return (s, r, d, {\"prob\" : p})\n",
        "\n",
        "# Mapping between directions and index number\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "\n",
        "# Maps for the two different environments\n",
        "MAPS = {\n",
        "    \"4x4\": [\n",
        "        \"SFFF\",\n",
        "        \"FHFH\",\n",
        "        \"FFFH\",\n",
        "        \"HFFG\"\n",
        "    ],\n",
        "    \"8x8\": [\n",
        "        \"SFFFFFFF\",\n",
        "        \"FFFFFFFF\",\n",
        "        \"FFFHFFFF\",\n",
        "        \"FHFFFHFF\",\n",
        "        \"FFFHFFFF\",\n",
        "        \"FFHFFFHF\",\n",
        "        \"FHFFHFHF\",\n",
        "        \"FFFHFFFG\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "class FrozenLakeEnv(DiscreteEnv):\n",
        "    \"\"\"\n",
        "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
        "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
        "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
        "    If you step into one of those holes, you'll fall into the freezing water.\n",
        "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
        "    you navigate across the lake and retrieve the disc.\n",
        "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
        "    The surface is described using a grid like the following\n",
        "\n",
        "        SFFF\n",
        "        FHFH\n",
        "        FFFH\n",
        "        HFFG\n",
        "\n",
        "    S : starting point, safe\n",
        "    F : frozen surface, safe\n",
        "    H : hole, fall to your doom\n",
        "    G : goal, where the frisbee is located\n",
        "\n",
        "    The episode ends when you reach the goal or fall in a hole.\n",
        "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {'render.modes': ['human', 'ansi']}\n",
        "\n",
        "    def __init__(self, desc=None, map_name=\"4x4\",is_slippery=True):\n",
        "        if desc is None and map_name is None:\n",
        "            raise ValueError('Must provide either desc or map_name')\n",
        "        elif desc is None:\n",
        "            desc = MAPS[map_name]\n",
        "        self.desc = desc = np.asarray(desc,dtype='c')\n",
        "        self.nrow, self.ncol = nrow, ncol = desc.shape\n",
        "\n",
        "        nA = 4 # number of actions\n",
        "        nS = nrow * ncol # number of states\n",
        "\n",
        "        isd = np.array(desc == b'S').astype('float64').ravel()\n",
        "        isd /= isd.sum()\n",
        "\n",
        "        P = {s : {a : [] for a in range(nA)} for s in range(nS)}\n",
        "\n",
        "        def to_s(row, col):\n",
        "            return row*ncol + col\n",
        "        def inc(row, col, a):\n",
        "            if a==0: # left\n",
        "                col = max(col-1,0)\n",
        "            elif a==1: # down\n",
        "                row = min(row+1,nrow-1)\n",
        "            elif a==2: # right\n",
        "                col = min(col+1,ncol-1)\n",
        "            elif a==3: # up\n",
        "                row = max(row-1,0)\n",
        "            return (row, col)\n",
        "\n",
        "        for row in range(nrow):\n",
        "            for col in range(ncol):\n",
        "                s = to_s(row, col)\n",
        "                for a in range(4):\n",
        "                    li = P[s][a]\n",
        "                    letter = desc[row, col]\n",
        "                    if letter in b'GH':\n",
        "                        li.append((1.0, s, 0, True))\n",
        "                    else:\n",
        "                        if is_slippery:\n",
        "                            for b in [(a-1)%4, a, (a+1)%4]:\n",
        "                                newrow, newcol = inc(row, col, b)\n",
        "                                newstate = to_s(newrow, newcol)\n",
        "                                newletter = desc[newrow, newcol]\n",
        "                                done = bytes(newletter) in b'GH'\n",
        "                                rew = float(newletter == b'G')\n",
        "                                li.append((0.8 if b==a else 0.1, newstate, rew, done))\n",
        "                        else:\n",
        "                            newrow, newcol = inc(row, col, a)\n",
        "                            newstate = to_s(newrow, newcol)\n",
        "                            newletter = desc[newrow, newcol]\n",
        "                            done = bytes(newletter) in b'GH'\n",
        "                            rew = float(newletter == b'G')\n",
        "                            li.append((1.0, newstate, rew, done))\n",
        "\n",
        "        super(FrozenLakeEnv, self).__init__(nS, nA, P, isd)\n",
        "\n",
        "    def _render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            return\n",
        "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "\n",
        "        row, col = self.s // self.ncol, self.s % self.ncol\n",
        "        desc = self.desc.tolist()\n",
        "        desc = [[c.decode('utf-8') for c in line] for line in desc]\n",
        "        desc[row][col] = utils.colorize(desc[row][col], \"red\", highlight=True)\n",
        "        if self.lastaction is not None:\n",
        "            outfile.write(\"  ({})\\n\".format([\"Left\",\"Down\",\"Right\",\"Up\"][self.lastaction]))\n",
        "        else:\n",
        "            outfile.write(\"\\n\")\n",
        "        outfile.write(\"\\n\".join(''.join(line) for line in desc)+\"\\n\")\n",
        "\n",
        "        return outfile\n",
        "\n",
        "\n",
        "# coding: utf-8\n",
        "\"\"\"Defines some frozen lake maps.\"\"\"\n",
        "\n",
        "register(\n",
        "    id='Deterministic-4x4-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '4x4',\n",
        "            'is_slippery': False})\n",
        "\n",
        "register(\n",
        "    id='Deterministic-8x8-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '8x8',\n",
        "            'is_slippery': False})\n",
        "\n",
        "register(\n",
        "    id='Stochastic-4x4-FrozenLake-v0',\n",
        "    entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
        "    kwargs={'map_name': '4x4',\n",
        "            'is_slippery': True})\n",
        "\n",
        "def render_single(env, policy, max_steps=100):\n",
        "    \"\"\"\n",
        "        Allows you to watch your agent navigate the lake\n",
        "        Parameters\n",
        "        ----------\n",
        "        env: gym.core.Environment\n",
        "        Environment to play on. Must have nS, nA, and P as\n",
        "        attributes.\n",
        "        Policy: np.array of shape [env.nS]\n",
        "        The action to take at a given state\n",
        "    \"\"\"\n",
        "\n",
        "    episode_reward = 0\n",
        "    ob = env.reset()\n",
        "    for t in range(max_steps):\n",
        "        if print_episode_run:\n",
        "            env.render()\n",
        "            time.sleep(0.25)\n",
        "        a = policy[ob]\n",
        "        ob, rew, done, _ = env.step(a)\n",
        "        episode_reward += rew\n",
        "        if done:\n",
        "            if print_episode_run:\n",
        "                env.render()\n",
        "            break\n",
        "    if not done:\n",
        "        print(\"The agent didn't reach a terminal state in {} steps.\".format(max_steps))\n",
        "    else:\n",
        "        print(\"Agent reached terminal state in {steps_taken} steps gathering an episodic reward of: {over_reward}\".format(over_reward = episode_reward, steps_taken = t + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byok33X1AazJ"
      },
      "source": [
        "##<font size = 5pt>General Parameters</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-LNthuqo6IY"
      },
      "source": [
        "* env.P - `nested dictionary` :\n",
        "Use like this:\n",
        "P[state][action] = (probability, nextstate, reward, terminal) \\\n",
        "<font size = 2pt> If environment is stochastic each P[state][action] could have multiple 4-tuples whose probability all add up to 1</font>\n",
        "\n",
        "  * probability `float` : the probability of transitioning from \"state\" to \"nextstate\" with \"action\" : $P(s^{\\prime} | s, a)$\n",
        "\n",
        "  * nextstate `int` : the state we transition to : $s^{\\prime}$ in range $[0,$ env.nS $ - 1]$\n",
        "\n",
        "  * reward `int` : the reward for transitioning from \"state\" to \"nextstate\" with \"action\" :\n",
        "$ R(s, a, s^{\\prime})=   \\left\\{\n",
        "\\begin{array}{ll}\n",
        "     1 & \\text{if transitioned to goal $(s^{\\prime} = $ 'G')} \\\\\n",
        "     0 & \\text{otherwise} \\\\\n",
        "\\end{array}\n",
        "\\right.  $\n",
        "\n",
        "  * terminal `boolean` : marks the terminal state : terminal =  $   \\left\\{\n",
        "\\begin{array}{ll}\n",
        "     True & \\text{if on goal or hole $(s = $ 'G' $ \\lor $ 'H')}\\\\\n",
        "     False & \\text{otherwise}\\\\\n",
        "\\end{array}\n",
        "\\right.  $\n",
        "\n",
        "* env.nS `int` : the number of states in the environment : $|\\mathcal{S}|$\n",
        "\n",
        "* env.nA `int` : the number of actions in the environment : $|\\mathcal{A}|$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvsStwCWAj3q"
      },
      "source": [
        "##<font size = 5pt>Policy Evaluation</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeXw_R3oyqMK"
      },
      "source": [
        "<font size = 4pt>Parameters</font>\n",
        "\n",
        "* env `gym.env` : gym environment <font size = 2pt>(must have P, nS, nA)</font>\n",
        "\n",
        "* policy `np.array[nS]` : the policy to evaluate : $\\pi$\n",
        "\n",
        "* gamma `float` : Discount factor : $\\gamma$ in range $[0,1)$\n",
        "\n",
        "* tol `float` : policy convergence tolerance : $max_{s \\in \\mathcal{S}} |V^{\\pi}_{k}(s) - V^{\\pi}_{k - 1}(s)| < tol$\n",
        "\n",
        "* max_iterations `int` : Max number of iterations allowed before stopping (incase convergence takes a while)\n",
        "\n",
        "<font size = 4pt>Returns</font>\n",
        "* value_function `np.ndarray[nS]` : The value function of the given policy : $V^{\\pi}(s)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "creuM4aIo3oH"
      },
      "source": [
        "# Evaluate the given policy by converging the state-value function. Returns array containing the value of every state\n",
        "def policy_evaluation(env, policy, gamma=0.9, tol=1e-3, max_iterations=int(1e3)):\n",
        "\n",
        "    # Initialize V(s) = 0 for all s\n",
        "    value_function = np.zeros(env.nS)\n",
        "\n",
        "    # Did a max-iteration just incase\n",
        "    for k in range(max_iterations):\n",
        "        delta = 0\n",
        "        # Loop through all states\n",
        "        for state in range(env.nS):\n",
        "            new_value = 0\n",
        "            # Calculate V(s)\n",
        "            for probability, nextstate, reward, terminal in env.P[state][policy[state]]:\n",
        "                new_value += probability * (reward + gamma * value_function[nextstate])\n",
        "            # Check difference with tolerance to see if converged\n",
        "            delta = max(delta, abs(value_function[state] - new_value))\n",
        "            value_function[state] = new_value\n",
        "        if delta < tol:\n",
        "            # Value function converged\n",
        "            break\n",
        "\n",
        "    if debug:\n",
        "        print(\"Policy Evaluation converged in {} iterations\".format(k))\n",
        "    return value_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-voMn4SAqS2"
      },
      "source": [
        "##<font size = 5pt>Policy Improvement</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHI-kyL2OnrF"
      },
      "source": [
        "<font size = 4pt>Parameters</font>\n",
        "\n",
        "* env `gym.env` : gym environment <font size = 2pt>(must have P, nS, nA)</font>\n",
        "\n",
        "* value_from_policy `np.ndarray[nS]` : value function corresponding to the given policy : $V^{\\pi}(s)$\n",
        "\n",
        "* policy `np.array[nS]` : the policy to evaluate : $\\pi$\n",
        "\n",
        "* gamma `float` : Discount factor : $\\gamma$ in range $[0,1)$\n",
        "\n",
        "<font size = 4pt>Returns</font>\n",
        "* stable `boolean` : flags a change in the policy : stable $   \\left\\{\n",
        "\\begin{array}{ll}\n",
        "     True & \\text{if policy never changed}\\\\\n",
        "     False & \\text{otherwise}\\\\\n",
        "\\end{array}\n",
        "\\right.  $\n",
        "\n",
        "* new_policy: `np.ndarray[nS]` : An improved policy with the optimal actions mapped, $\\pi^{\\prime}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgBHQl7JO10m"
      },
      "source": [
        "# Improve a given policy by using the state-action value function. Return boolean that flags if the policy changed and the policy\n",
        "def policy_improvement(env, value_from_policy, policy, gamma=0.9):\n",
        "\n",
        "\t# Track policy stability\n",
        "\tstable = True\n",
        "\n",
        "\t# Loop through all states\n",
        "\tfor state in range(env.nS):\n",
        "\t\t# Track previous action this policy used in this state\n",
        "\t\tprev_action = policy[state]\n",
        "\t\t# Calculate Q(s,a) for all actions and track max-Q(s,a) (max/'best' action)\n",
        "\t\tmax_state_action_value = -1\n",
        "\t\tfor action in range(env.nA):\n",
        "\t\t\tstate_action_value = 0\n",
        "\t\t\tfor probability, nextstate, reward, terminal in env.P[state][action]:\n",
        "\t\t\t\tstate_action_value += probability * (reward + gamma * value_from_policy[nextstate])\n",
        "\t\t\tif (state_action_value > max_state_action_value):\n",
        "\t\t\t\tmax_state_action_value = state_action_value\n",
        "\t\t\t\tpolicy[state] = action\n",
        "\t\tstable = False if prev_action != policy[state] else stable\n",
        "\n",
        "\treturn stable, policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xftSSg7RAujr"
      },
      "source": [
        "##<font size = 5pt>Policy Iteration</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idqwsOLJO27Y"
      },
      "source": [
        "<font size = 4pt>Parameters</font>\n",
        "\n",
        "* env `gym.env` : gym environment <font size = 2pt>(must have P, nS, nA)</font>\n",
        "\n",
        "* gamma `float` : Discount factor : $\\gamma$ in range $[0,1)$\n",
        "\n",
        "* tol `float` : policy convergence tolerance for policy_evaluation()\n",
        "\n",
        "* max_iterations `int` : Max number of iterations allowed in policy_evaluation() before stopping\n",
        "\n",
        "<font size = 4pt>Returns</font>\n",
        "* value_function `np.ndarray[nS]` : The value function of the optimal policy :  $V^*(s)$\n",
        "\n",
        "* policy: `np.ndarray[nS]` : The optimal policy : $\\pi^*(s)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bxHE3HXPGK7"
      },
      "source": [
        "# Iteratively find the optimal policy and state-value function function by policy evaluation + policy improvement loops\n",
        "def policy_iteration(env, gamma=0.9, tol=10e-3, max_iterations=int(1e3)):\n",
        "\titer_count = 0\n",
        "\n",
        "\t# Initialize V(s) for all s\n",
        "\tvalue_function = np.zeros(env.nS)\n",
        "    # Initialize pi(s) for all s\n",
        "\tpolicy = np.zeros(env.nS, dtype=int)\n",
        "\n",
        "    # Track policy stability\n",
        "\tstable = False\n",
        "\twhile(not stable):\n",
        "\t\t# Evaluate the policy\n",
        "\t\tvalue_function = policy_evaluation(env, policy, gamma, tol, max_iterations)\n",
        "        # Improve the policy\n",
        "\t\tstable, policy = policy_improvement(env, value_function, policy, gamma)\n",
        "\t\titer_count += 1\n",
        "\n",
        "\tif debug:\n",
        "\t\tprint('\\nPolicy Iteration converged in {} iterations'.format(iter_count))\n",
        "\treturn value_function, policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZlW1B_jAzNZ"
      },
      "source": [
        "##<font size = 5pt>Value Iteration</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c11fvd6PPG5l"
      },
      "source": [
        "<font size = 4pt>Parameters</font>\n",
        "\n",
        "* env `gym.env` : gym environment <font size = 2pt>(must have P, nS, nA)</font>\n",
        "\n",
        "* gamma `float` : Discount factor : $\\gamma$ in range $[0,1)$\n",
        "\n",
        "* tol `float` : policy convergence tolerance for policy_evaluation()\n",
        "\n",
        "* max_iterations `int` : Max number of iterations allowed in policy_evaluation() before stopping\n",
        "\n",
        "<font size = 4pt>Returns</font>\n",
        "* value_function `np.ndarray[nS]` : The value function of the optimal policy :  $V^*(s)$\n",
        "\n",
        "* policy: `np.ndarray[nS]` : The optimal policy : $\\pi^*(s)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WODNL2HPLMD"
      },
      "source": [
        "def value_iteration(env, gamma=0.9, tol=1e-3, max_iterations=int(1e3)):\n",
        "\n",
        "\t# Initialize V(s) = 0 for all s\n",
        "\tvalue_function = np.zeros(env.nS)\n",
        "    # Initialize pi(s) = 0 for all s\n",
        "\tpolicy = np.zeros(env.nS, dtype=int)\n",
        "\n",
        "\t# Did a max-iteration just incase\n",
        "\tfor k in range(max_iterations):\n",
        "\t\tdelta = 0\n",
        "        # Loop through all states\n",
        "\t\tfor state in range(env.nS):\n",
        "\t\t\tprev_value = value_function[state]\n",
        "\t\t\t# loop through all actions\n",
        "\t\t\tfor action in range(env.nA):\n",
        "\t\t\t\t# calculate Q(s,a)\n",
        "\t\t\t\tstate_action_value = 0\n",
        "\t\t\t\tfor probability, nextstate, reward, terminal in env.P[state][action]:\n",
        "\t\t\t\t\tstate_action_value += probability * (reward + gamma * value_function[nextstate])\n",
        "\t            # Update V(s) with max-Q(s,a) and pi(s) with a from max-Q(s,a)\n",
        "\t\t\t\tif (state_action_value > value_function[state]):\n",
        "\t\t\t\t\tvalue_function[state] = state_action_value\n",
        "\t\t\t\t\tpolicy[state] = action\n",
        "\t\t\tdelta = max(delta, abs(prev_value - value_function[state]))\n",
        "\t\tif delta < tol:\n",
        "\t\t\t# value function converged\n",
        "\t\t\tbreak\n",
        "\n",
        "\tif debug:\n",
        "\t\tprint('Value Iteration converged in {} iterations.'.format(k))\n",
        "\treturn value_function, policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sBYFx3twUw2"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR8sAg-M9TKO"
      },
      "source": [
        "#@title Settings <font size = 2pt>(run cell after change)</font>\n",
        "\n",
        "debug = True #@param [True, False] {type:'boolean'}\n",
        "print_episode_run = True #@param [True, False] {type:'boolean'}\n",
        "frozen_lake_id =  'Deterministic-4x4-FrozenLake-v0' #@param ['Deterministic-4x4-FrozenLake-v0', 'Deterministic-8x8-FrozenLake-v0', 'Stochastic-4x4-FrozenLake-v0']\n",
        "dim = 4\n",
        "if (frozen_lake_id == 'Deterministic-8x8-FrozenLake-v0'):\n",
        "    dim = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8Epsn5sLct8"
      },
      "source": [
        "# Test Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXBhH_PsLl-6",
        "outputId": "83582930-c741-4661-aa9f-211e54ca7637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env = gym.make(frozen_lake_id)\n",
        "\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Policy Iteration\\n\" + \"-\"*25)\n",
        "V_pi, p_pi = policy_iteration(env, gamma=0.9, tol=1e-3)\n",
        "if debug:\n",
        "    print ('\\nOptimal Value Function:\\n', V_pi.reshape(dim,dim), '\\n\\nOptimal Policy:\\n', p_pi.reshape(dim,dim))\n",
        "render_single(env, p_pi, 100)\n",
        "\n",
        "print(\"\\n\" + \"-\"*25 + \"\\nBeginning Value Iteration\\n\" + \"-\"*25)\n",
        "V_vi, p_vi = value_iteration(env, gamma=0.9, tol=1e-3)\n",
        "if debug:\n",
        "    print ('\\nOptimal Value Function:\\n', V_vi.reshape(dim,dim), '\\n\\nOptimal Policy:\\n', p_vi.reshape(dim,dim))\n",
        "render_single(env, p_vi, 100)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "-------------------------\n",
            "Beginning Policy Iteration\n",
            "-------------------------\n",
            "Policy Evaluation converged in 0 iterations\n",
            "Policy Evaluation converged in 1 iterations\n",
            "Policy Evaluation converged in 2 iterations\n",
            "Policy Evaluation converged in 3 iterations\n",
            "Policy Evaluation converged in 4 iterations\n",
            "Policy Evaluation converged in 5 iterations\n",
            "Policy Evaluation converged in 6 iterations\n",
            "\n",
            "Policy Iteration converged in 7 iterations\n",
            "\n",
            "Optimal Value Function:\n",
            " [[0.59  0.656 0.729 0.656]\n",
            " [0.656 0.    0.81  0.   ]\n",
            " [0.729 0.81  0.9   0.   ]\n",
            " [0.    0.9   1.    0.   ]] \n",
            "\n",
            "Optimal Policy:\n",
            " [[1 2 1 0]\n",
            " [1 0 1 0]\n",
            " [2 1 1 0]\n",
            " [0 2 2 0]]\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Agent reached terminal state in 6 steps gathering an episodic reward of: 1.0\n",
            "\n",
            "-------------------------\n",
            "Beginning Value Iteration\n",
            "-------------------------\n",
            "Value Iteration converged in 6 iterations.\n",
            "\n",
            "Optimal Value Function:\n",
            " [[0.59  0.656 0.729 0.656]\n",
            " [0.656 0.    0.81  0.   ]\n",
            " [0.729 0.81  0.9   0.   ]\n",
            " [0.    0.9   1.    0.   ]] \n",
            "\n",
            "Optimal Policy:\n",
            " [[1 2 1 0]\n",
            " [1 0 1 0]\n",
            " [2 1 1 0]\n",
            " [0 2 2 0]]\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Agent reached terminal state in 6 steps gathering an episodic reward of: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}