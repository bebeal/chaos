{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9qot-sVpliM"
      },
      "source": [
        "# Imports and initializations\n",
        "import os, math\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gym import spaces\n",
        "import cv2\n",
        "from collections import deque\n",
        "import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from gym.wrappers import atari_preprocessing, TransformReward\n",
        "\n",
        "# GPU OR CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kBgVcr7pyTM"
      },
      "source": [
        "# Hyperparameters\n",
        "num_frames = 2000000\n",
        "batch_size = 32\n",
        "memory_capacity = 1000000\n",
        "learning_rate = 0.00025\n",
        "gamma = 0.99\n",
        "replay_initial = 50000\n",
        "update_freq = 10000\n",
        "checkpoint_freq = 10000\n",
        "epsilon = 1\n",
        "epsilon_start = epsilon\n",
        "epsilon_end = 0.01\n",
        "anneal_over_frames = 1000000\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFMVa8GEpyFc"
      },
      "source": [
        "# DuelingDQN with DDQN algorithm\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, num_actions, learning_rate):\n",
        "        super(DDQN, self).__init__()\n",
        "\n",
        "        self.num_actions = num_actions\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "\n",
        "            nn.Conv2d(1, 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.adv = nn.Sequential(\n",
        "            nn.Linear(self.feature_size(), 512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, self.num_actions)\n",
        "        )\n",
        "\n",
        "        self.val = nn.Sequential(\n",
        "            nn.Linear(self.feature_size(), 512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def feature_size(self):\n",
        "        return self.features(torch.zeros(1, *(1,84,84))).view(1, -1).size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        adv = self.adv(x)\n",
        "        val = self.val(x)\n",
        "        return val + adv - adv.mean()\n",
        "\n",
        "    def select_best_action(self, state, writer):\n",
        "        state_ = torch.tensor(state,dtype=torch.float32,device=device).unsqueeze(0)\n",
        "        q_values = self.forward(state_).detach()\n",
        "        q_action_dict = {'action' + str(i):x for i,x in enumerate(q_values[0].tolist())}\n",
        "        writer.add_scalars('Q value of each action', q_action_dict, frame_idx)\n",
        "        writer.flush()\n",
        "        return q_values.max(1)[1].item()\n",
        "\n",
        "    def save_variables(self, model_file):\n",
        "        torch.save({\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "        }, os.path.join(model_file, 'DDQN.pt'))\n",
        "\n",
        "\n",
        "    def load_variables(self, PATH):\n",
        "        checkpoint = torch.load(PATH)\n",
        "        self.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.eval()\n",
        "\n",
        "\n",
        "    def turn_on_training(self):\n",
        "        self.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqJL4yzUpx7l"
      },
      "source": [
        "# ReplayMemory buffer for agent\n",
        "class ReplayMemory():\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        state = np.expand_dims(state, 0)\n",
        "        next_state = np.expand_dims(next_state, 0)\n",
        "        if len(self.memory) >= self.capacity:\n",
        "            self.memory.pop(0)\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def get_batch_sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.memory, batch_size))\n",
        "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Sb1jOW4px3S"
      },
      "source": [
        "# Agent\n",
        "class DDQNAgent():\n",
        "    def __init__(self, env, epsilon, learning_rate, gamma, capacity):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = gamma\n",
        "        self.dqn = DDQN(env.action_space.n, learning_rate).to(device)\n",
        "        self.target_dqn = DDQN(env.action_space.n, learning_rate).to(device)\n",
        "        self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
        "        self.agent_memory = ReplayMemory(capacity)\n",
        "\n",
        "    def get_action(self, state, writer):\n",
        "        writer.add_scalar('epislon', self.epsilon, frame_idx)\n",
        "        writer.flush()\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            action = random.choice(range(env.action_space.n))\n",
        "        else:\n",
        "            action = self.dqn.select_best_action(state, writer)\n",
        "        return action\n",
        "\n",
        "    def push_to_memory(self, state, action, reward, next_state, done):\n",
        "        self.agent_memory.push(state, action, reward, next_state, done)\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
        "\n",
        "    def compute_loss(self, state, action, reward, next_state, done):\n",
        "        states = torch.tensor(state, dtype=torch.float32, device=device)\n",
        "        next_states = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
        "        actions = torch.tensor(action, dtype=torch.long, device=device)\n",
        "        rewards = torch.tensor(reward, dtype=torch.float32, device=device)\n",
        "        dones = torch.tensor(done, dtype=torch.float32, device=device)\n",
        "\n",
        "        q_values = self.dqn.forward(states)\n",
        "        next_q_values = self.dqn.forward(next_states).detach()\n",
        "        next_q_state_values = self.target_dqn(next_states)\n",
        "\n",
        "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
        "        expected_q_value = rewards + self.gamma * next_q_value * (1 - dones)\n",
        "\n",
        "        loss = nn.SmoothL1Loss()\n",
        "        loss = loss(expected_q_value, q_value)\n",
        "\n",
        "        self.dqn.optimizer.zero_grad()\n",
        "        # Calculate gradients\n",
        "        loss.backward()\n",
        "        self.dqn.optimizer.step()\n",
        "\n",
        "        return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM9TVVG7pxzN"
      },
      "source": [
        "# Game settings and initialization\n",
        "game = \"PongNoFrameskip-v4\"\n",
        "render_video = True\n",
        "env = gym.make(game)\n",
        "env = atari_preprocessing.AtariPreprocessing(env)\n",
        "agent = DDQNAgent(env, epsilon_start, learning_rate, gamma, memory_capacity)\n",
        "\n",
        "def get_epsilon(frame):\n",
        "    return (epsilon_end + max(0, (epsilon_start - epsilon_end) * (anneal_over_frames - max(0, frame - 0))/anneal_over_frames))\n",
        "\n",
        "all_rewards = []\n",
        "episode_reward = 0\n",
        "state = env.reset()\n",
        "state = np.expand_dims(state, 0)\n",
        "# if render_video:\n",
        "    # env.render()\n",
        "\n",
        "# FOR LOGGING\n",
        "PATH_to_log_dir = 'C:/Users/bnoah/PycharmProjects/DM-DQN-Atari/Data/DDQN/'\n",
        "# Declare Tensorboard writer\n",
        "timestr = time.strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter(PATH_to_log_dir + timestr)\n",
        "model_file = os.path.join(PATH_to_log_dir, game[:10]+'model_test')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT0D_UDFpxok"
      },
      "source": [
        "# Train\n",
        "for frame_idx in range(1, num_frames + 1):\n",
        "    agent.epsilon = get_epsilon(frame_idx)\n",
        "\n",
        "    action = agent.get_action(state, writer)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    next_state = np.expand_dims(next_state, 0)\n",
        "    agent.push_to_memory(state, action, reward, next_state, done)\n",
        "\n",
        "    # if render_video:\n",
        "        # env.render()\n",
        "\n",
        "    state = next_state\n",
        "    episode_reward += reward\n",
        "\n",
        "    if done:\n",
        "        state = env.reset()\n",
        "        state = np.expand_dims(state, 0)\n",
        "        all_rewards.append(episode_reward)\n",
        "        num_episodes = len(all_rewards)\n",
        "        writer.add_scalar('Reward of each episode', episode_reward, num_episodes)\n",
        "        writer.add_scalar('Average Reward All Time', np.mean(all_rewards), frame_idx)\n",
        "        if num_episodes % 10 == 0:\n",
        "            writer.add_scalar('Average reward of past 10 episodes', np.mean(all_rewards[-10]), num_episodes // 10)\n",
        "        writer.flush()\n",
        "        episode_reward = 0\n",
        "\n",
        "    if len(agent.agent_memory.memory) > replay_initial:\n",
        "        loss = agent.compute_loss(*agent.agent_memory.get_batch_sample(batch_size))\n",
        "\n",
        "    if frame_idx % update_freq == 0:\n",
        "        agent.update_target()\n",
        "\n",
        "    # if (frame_idx > batch_size and frame_idx % checkpoint_freq == 0):\n",
        "        # agent.dqn.save_variables(model_file)\n",
        "\n",
        "writer.close()\n",
        "# agent.dqn.save_variables(model_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}